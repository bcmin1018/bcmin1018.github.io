---
layout: single
title: "[Pytorch] Sequence to sequence + Attention 코드리뷰"
categories: deeplearning
tag: [pytorch, deeplearning, nlp]
toc: true
---

# 1. 예제 데이터
* 먼저 실제 데이터를 준비된 언어 사전을 기반으로 숫자로 치환한다. 예를 들어 "나는 학교에 갑니다."와 같은 문자로된 데이터가 5, 1, 3 과 같은 숫자로 변형된다. 숫자로 변형되는 기준은 언어 사전에 따라 다르게 나타난다. 치환된 숫자는 학습 모델에 맞게 tensor 형태로 변환된다.


```python
for input_tensor, input_length, target_tensor, target_length in train_loader:
  input_tensor = input_tensor[0]
  target_tensor = target_tensor[0]
  input_length = input_length
  target_length = target_length
  break
```

* 언어사전에서 입/출력 값은 만드는 과정은 생략. 최종적으로 입력/출력 텐서값과 길이를 출력한다. 영어를 한국어로 번역하기 때문에 입력은 영어를 뜻하고 출력은 한국어를 의미한다.


```python
print("입력 텐서 값 ", input_tensor)
print("출력 텐서 값 ", target_tensor)
print("입력 텐서 길이 ", input_length)
print("출력 텐서 길이 ", target_length)
```

    입력 텐서 값  tensor([2, 3, 4, 5, 6, 5, 7, 8, 9, 1])
    출력 텐서 값  tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  1])
    입력 텐서 길이  tensor([10])
    출력 텐서 길이  tensor([10])
    

# 2. Encoder

Sequence to sequence 모델에 Attention을 적용한다는 의미는 encoder의 입력에 대한 각각의 output, 즉 모든 hidden_state 정보를 이용한다는 것이다. 각 hidden_state를 사용하여 어떤 단어에 집중 할지에 대해 파악한다.

![image](https://user-images.githubusercontent.com/101251439/167293777-526b0adf-24be-4b3c-84b4-6663afbbce76.png)

* Encoder에 필요한 입력 값을 만듭니다.
  * input_size: encoder의 입력 차원 (영어 언어 사전 길이에 해당된다., 한 문자를 원핫 인코딩하면 단어 사전 길이 만큼의 차원을 갖게 되기 때문이다.)




```python
eng_dic_size = eng_dic.n_words
input_size = eng_dic_size
hidden_size = 256
max_len = 10
print("encoder_input_size: ", input_size)
print("hidden_size: ", hidden_size)
print("max_len :", max_len)
```

    encoder_input_size:  2506
    hidden_size:  256
    max_len : 10
    

* Encoder 동작 방식 : 문자 입력 -> 임베딩 -> RNN 순서 -> context vector 및 모든 hidden state 생성
  1. encoder_hidden: 최종 output(context vector)를 담을 배열(텐서) 생성
  2. encoder_outputs: 모든 output을 담을 배열 (텐서) 생성 
  3. embedding layer 생성
  4. RNN layer 생성
  5. 영어 데이터 입력하기
  6. encoder_hidden, encoder_outputs 출력하기


```python
# encoder의 최종 output인 context vecotor을 담을 3차원 텐서 만들기 간단하게 0으로 만들기
encoder_hidden = torch.zeros(1,1,256)
encoder_hidden.shape
```




    torch.Size([1, 1, 256])




```python
# encoder의 각 output (hidden_state)을 담을 배열 생성
encoder_outputs = torch.zeros(max_len, hidden_size)
encoder_outputs.shape
```




    torch.Size([10, 256])



* 아래의 그림처럼 hello라는 단어의 중복값을 제거하여 h i e l o 알파뱃으로 언어사전을 만들었다. 이를 각각 원핫인코딩하여 input_size는 5가 된다. 따라서 언어사전 길이가 1000이라고 하면 원핫인코딩하면 한개의 단어를 1000개의 벡터로 표현할 수 있다. 

![image](https://user-images.githubusercontent.com/101251439/167286530-64ebeee5-1419-44b8-8158-75dbee3b7913.png)


```python
# embedding 층 만들기
encoder_embedding = nn.Embedding(input_size, hidden_size)
encoder_embedding
```




    Embedding(2506, 256)




```python
# RNN 층 생성
encoder_rnn = nn.GRU(hidden_size, hidden_size)
encoder_rnn
```




    GRU(256, 256)




```python
# 첫번째 단어만 넣고 임베딩 예시
for idx in range(input_length):
  input_tensor_step = input_tensor[idx]
  print("임베딩할 텐서 값 ", input_tensor_step)
  embedded = encoder_embedding(input_tensor_step).view(1,1,-1)
  # encoder_output, encoder_hidden 생성
  encoder_output, encoder_hidden = encoder_rnn(embedded, encoder_hidden)
  encoder_outputs[idx] = encoder_output[0,0]
  break
print("Output: ", encoder_outputs)
print(encoder_outputs.shape)
print("Hidden: ", encoder_hidden)
print(encoder_hidden.shape)
```

    임베딩할 텐서 값  tensor(2)
    Output:  tensor([[-0.1800,  0.1494,  0.0140,  ...,  0.4770,  0.0010, -0.0221],
            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
            ...,
            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
           grad_fn=<CopySlices>)
    torch.Size([10, 256])
    Hidden:  tensor([[[-0.1800,  0.1494,  0.0140,  0.2764,  0.1389, -0.0278, -0.3010,
               0.1817,  0.2420,  0.1900, -0.0707, -0.3067, -0.0982,  0.2321,
              -0.2731, -0.0454, -0.1016,  0.3294, -0.5924,  0.2126, -0.1507,
               0.3126,  0.2100, -0.1110, -0.0823, -0.3029,  0.1468, -0.0797,
               0.0947,  0.0559,  0.0426, -0.0602,  0.0185,  0.0092, -0.4035,
              -0.2461, -0.0476, -0.2472,  0.0834, -0.2411,  0.2606, -0.0691,
               0.2050, -0.2181, -0.0790,  0.0682,  0.1005, -0.1808, -0.1647,
               0.1259, -0.0410, -0.1839,  0.0115,  0.3885, -0.2289, -0.2041,
               0.1733, -0.0084,  0.2846, -0.0184,  0.3019, -0.2970,  0.5535,
               0.0980, -0.3433, -0.1224, -0.2115,  0.1178,  0.0301, -0.0668,
              -0.0397, -0.1132, -0.1494,  0.0535, -0.2167,  0.2412, -0.2081,
               0.4955, -0.0270, -0.1445, -0.1853,  0.2103,  0.3388, -0.2968,
              -0.1330,  0.0768,  0.4989, -0.0484, -0.4029, -0.1269,  0.0564,
              -0.3243,  0.0047,  0.1290,  0.1776, -0.0332, -0.0513, -0.0218,
               0.0267, -0.1571, -0.1792, -0.3675,  0.2644,  0.0772,  0.3557,
              -0.3299, -0.0054, -0.1035,  0.2775, -0.1968,  0.0759, -0.1255,
              -0.0128,  0.1889, -0.2033, -0.2439,  0.2388, -0.0286, -0.1213,
               0.0048,  0.0101,  0.0885,  0.0123,  0.2871,  0.0750, -0.1456,
              -0.1881,  0.1586,  0.0107, -0.3479, -0.2465,  0.2774, -0.0259,
               0.0391, -0.1211, -0.1808,  0.0082, -0.0587,  0.2629,  0.3814,
              -0.3298,  0.1043,  0.4248,  0.2067, -0.1668, -0.2046,  0.2080,
              -0.0942, -0.0143,  0.0637,  0.2092,  0.2351,  0.2753, -0.0332,
               0.0613,  0.0460, -0.4249,  0.1079,  0.2334, -0.0800, -0.1338,
               0.2255,  0.4444,  0.4346, -0.3019,  0.0246,  0.0762, -0.3047,
               0.3343, -0.2363,  0.2500, -0.3887, -0.2618, -0.1272, -0.2284,
              -0.4046, -0.0688,  0.2193, -0.2411, -0.1717, -0.3240,  0.3301,
              -0.2572, -0.0570,  0.2929,  0.2495,  0.1236, -0.0562, -0.0475,
               0.1131,  0.1302, -0.3648,  0.1296,  0.0016,  0.0122, -0.0774,
              -0.3178, -0.3019,  0.2626, -0.2439, -0.0958, -0.1098,  0.0509,
               0.3817,  0.1840,  0.5097,  0.0362,  0.1002,  0.1003,  0.0651,
              -0.0841,  0.1421,  0.2908,  0.3355, -0.0501, -0.2670, -0.1376,
              -0.0587,  0.0709,  0.3226,  0.3045, -0.1817,  0.1334, -0.4986,
              -0.2215, -0.1631, -0.0599,  0.4069,  0.2982,  0.1351, -0.0868,
               0.2630, -0.1269, -0.0319, -0.3779,  0.0871,  0.2299,  0.3386,
              -0.1205, -0.0677, -0.1339,  0.0583,  0.0488,  0.3315, -0.4491,
               0.1979, -0.1955,  0.5656,  0.4001, -0.0297,  0.3504,  0.1136,
              -0.2812,  0.4770,  0.0010, -0.0221]]], grad_fn=<StackBackward0>)
    torch.Size([1, 1, 256])
    

encoder를 지난 최종 output (context vector)은 디코더의 첫번째 입력되는 hidden state이다.

![image](https://user-images.githubusercontent.com/101251439/167286811-38f4696b-6f06-4fba-866b-1abf9bd36af5.png)


```python
# 문장 전체 사용
for idx in range(input_length):
  input_tensor_step = input_tensor[idx]
  print("임베딩할 텐서 값 ", input_tensor_step)
  embedded = encoder_embedding(input_tensor_step).view(1,1,-1)
  # encoder_output, encoder_hidden 생성
  encoder_output, encoder_hidden = encoder_rnn(embedded, encoder_hidden)
  encoder_outputs[idx] = encoder_output[0,0]
  
print("Output: ", encoder_outputs)
print(encoder_outputs.shape)
print("Hidden: ", encoder_hidden)
print(encoder_hidden.shape)
```

    임베딩할 텐서 값  tensor(2)
    임베딩할 텐서 값  tensor(3)
    임베딩할 텐서 값  tensor(4)
    임베딩할 텐서 값  tensor(5)
    임베딩할 텐서 값  tensor(6)
    임베딩할 텐서 값  tensor(5)
    임베딩할 텐서 값  tensor(7)
    임베딩할 텐서 값  tensor(8)
    임베딩할 텐서 값  tensor(9)
    임베딩할 텐서 값  tensor(1)
    Output:  tensor([[-0.2101,  0.1963,  0.0205,  ...,  0.6142,  0.0037, -0.0304],
            [-0.3297,  0.0476,  0.2311,  ...,  0.2259, -0.0043,  0.4193],
            [-0.4934, -0.4131, -0.1565,  ..., -0.3029,  0.4336,  0.4013],
            ...,
            [-0.0591,  0.5687, -0.3574,  ...,  0.0246,  0.1674, -0.5449],
            [ 0.1991, -0.0494, -0.3080,  ..., -0.4123,  0.5717, -0.5242],
            [ 0.1855, -0.3274, -0.1775,  ..., -0.4647, -0.0044, -0.4288]],
           grad_fn=<CopySlices>)
    torch.Size([10, 256])
    Hidden:  tensor([[[ 1.8553e-01, -3.2737e-01, -1.7746e-01,  2.9361e-01, -2.7654e-01,
              -1.6257e-02, -9.3238e-02, -2.9453e-01, -3.6550e-01,  5.1483e-01,
              -4.0575e-01, -2.0769e-01, -1.6290e-01,  2.2070e-02, -2.6099e-01,
              -2.4117e-01, -3.5822e-01,  4.1358e-02, -1.2331e-01,  4.3622e-01,
               5.7500e-02, -3.6290e-01, -3.2169e-01,  2.4425e-01, -4.3693e-02,
              -1.2849e-01,  3.4084e-01,  3.0567e-01,  7.5472e-02,  7.5786e-02,
              -1.7097e-02, -2.7780e-02, -1.0886e-01,  7.9470e-02,  7.5075e-02,
              -4.5910e-01,  3.1378e-02,  3.5593e-01, -8.4244e-02, -8.6812e-02,
              -1.5236e-01, -1.3729e-01, -1.2754e-01,  1.9763e-01,  1.7052e-02,
               3.7005e-01,  1.2101e-01,  1.6412e-01, -2.0437e-01, -2.6039e-01,
              -4.8333e-01,  2.3079e-01,  1.1424e-02,  1.4952e-01, -7.5544e-02,
              -4.1887e-01,  2.0382e-01, -3.5353e-01,  1.1993e-01,  3.8428e-01,
               6.6012e-02, -2.4601e-01,  1.0371e-01,  1.9052e-02, -6.6556e-02,
              -4.0570e-01,  9.0233e-02, -7.5007e-02,  3.6542e-01, -2.5494e-01,
               5.0446e-01,  4.4830e-01, -1.7160e-01,  2.0022e-01,  5.0578e-01,
              -8.6898e-02,  2.6248e-01,  5.7014e-01, -3.3528e-01, -2.7163e-01,
               1.4251e-01,  2.4656e-01,  2.0597e-01, -3.8794e-01,  2.1079e-01,
              -1.3061e-02,  2.8287e-01, -1.1618e-01,  8.0891e-02,  3.1247e-03,
              -1.6827e-01, -2.1751e-02,  2.0819e-01, -2.7487e-01, -9.4698e-02,
               5.1001e-02, -1.3804e-01,  2.1864e-01, -8.7541e-02,  9.2098e-02,
               1.5366e-01,  1.2446e-01, -3.1948e-01, -7.9883e-02, -2.5002e-01,
               1.7433e-01,  3.7732e-01, -5.4707e-01,  3.6940e-02, -2.1193e-01,
              -3.6361e-01,  2.1733e-02, -2.7215e-01, -8.4042e-03, -5.0374e-01,
               5.8897e-01, -8.9980e-02, -3.5412e-01,  3.4306e-01,  7.2969e-02,
               3.9981e-01, -1.9689e-01, -3.6813e-01, -3.0297e-01, -2.4809e-01,
              -1.8404e-01, -3.5193e-01, -2.4280e-01,  5.2934e-01,  2.0030e-01,
               5.1825e-01,  7.3710e-02, -1.6934e-01,  3.9468e-01,  3.3090e-01,
               1.0285e-01,  1.0116e-01,  4.8757e-02,  7.2215e-02, -2.5305e-01,
               3.2151e-01,  1.5773e-01,  7.0490e-02,  3.7154e-02, -2.2654e-01,
               1.9841e-01, -1.5935e-01, -5.9520e-03,  2.6613e-01,  1.0813e-01,
              -2.3759e-01, -1.1725e-01,  1.0883e-01,  5.0219e-01, -1.0235e-01,
              -1.5335e-02, -4.0385e-02, -5.1763e-03,  2.5441e-01,  3.4116e-01,
               2.0091e-01,  3.5100e-01, -7.1971e-01, -4.6089e-01, -3.6717e-01,
              -4.0999e-02,  3.8707e-01, -2.5014e-01,  1.3851e-01,  5.2093e-01,
              -2.6153e-01, -3.0655e-01, -3.4004e-01, -2.6730e-01,  1.9310e-01,
              -3.7675e-02, -1.5948e-01,  3.9838e-01, -6.4142e-02,  1.8905e-01,
              -3.0410e-02, -3.4934e-01, -2.1973e-01,  1.3757e-01, -5.5193e-01,
               3.0144e-01, -1.5491e-01,  3.9494e-01, -1.9529e-01,  8.4889e-02,
              -2.4529e-01, -1.1893e-01,  6.2899e-02,  4.7063e-01, -3.3886e-01,
              -4.0528e-01,  7.6353e-02, -1.9737e-01,  3.5344e-01, -9.3965e-02,
              -2.2042e-01, -2.5712e-01, -4.1075e-01,  1.0953e-01,  1.7969e-01,
              -5.0786e-02,  3.3239e-04,  3.8911e-01, -2.4529e-01, -1.0148e-01,
              -4.6527e-02,  5.1050e-01,  1.1269e-02,  5.1544e-01,  2.5713e-01,
               1.4458e-01,  1.4033e-01, -2.0526e-01,  5.3335e-01,  2.2515e-01,
               3.1177e-01,  5.5605e-01, -5.4880e-01, -4.4740e-02,  1.1625e-01,
              -4.0010e-01,  4.5902e-01, -1.5089e-02,  1.4426e-01, -4.0610e-01,
               1.0105e-01,  3.6140e-01,  4.3310e-01,  8.4004e-02, -6.2668e-01,
              -1.6764e-03, -1.0335e-03,  6.7559e-02, -2.3461e-01, -2.0065e-01,
              -4.5580e-01, -1.6117e-02,  6.5196e-02, -2.4422e-01, -2.0991e-01,
              -1.2629e-01, -3.1711e-01, -1.6199e-01,  1.9908e-01,  2.7948e-01,
              -2.5601e-03,  4.6142e-01,  1.8507e-01, -4.6472e-01, -4.4442e-03,
              -4.2875e-01]]], grad_fn=<StackBackward0>)
    torch.Size([1, 1, 256])
    

# 3. Decoder

디코더 과정에서 인코더의 hidden state를 이용해서 Attention 기법을 사용합니다.
Attention 층을 거친 최종 output은 디코더 입력값과 연산을 통해 다음 단어를 예측합니다.

![image](https://user-images.githubusercontent.com/101251439/167294177-0089f034-8ca5-4185-80d4-4f7ecbc75e28.png)

* 디코더에 필요한 입력값을 만듭니다.
  * input_size & output_size: decoder의 입력 차원 또는 decoder의 출력 차원 (한국어 언어 사전 길이에 해당된다.) 




```python
kor_dic_size = kor_dic.n_words
output_size = kor_dic_size
hidden_size = 256
print("decoder_hiddenstate: ", hidden_size)
print("decoder_input_size or output_size: ", output_size)
```

    decoder_hiddenstate:  256
    decoder_input_size or output_size:  3241
    

  * Decoder 동작 방식 : SOS 토큰 입력 -> 임배딩 -> dropout -> Attention -> RNN -> output

  1. Decoder 첫 입력값 SOS 토큰 백터 만들기
  2. 디코더 임배딩, 드롭아웃 층 생성
  3. 디코더 RNN 층 생성
  4. 어텐션 층 생성
  


* 디코더에 필요한 층 생성


```python
#decoder_embedding
decoder_embedding = nn.Embedding(output_size, hidden_size)
decoder_embedding
```




    Embedding(3241, 256)




```python
#dropout layer
dropout = nn.Dropout(0.1)
```


```python
#rnn layer 생성
decoder_rnn = nn.GRU(hidden_size, hidden_size)
decoder_rnn
```




    GRU(256, 256)



* 디코더의 다음 단어를 생성하기 위해 인코더에 입력된 단어 중 어느 단어에 집중을 할까? 집중해야할 단어들에 점수(Attention Score)를 부여하기 위해 위한 어텐션 층을 생성한다.


```python
#attention layer: 512 길이를 10으로 축소한다.
attn = nn.Linear(hidden_size * 2, max_len)
attn
```




    Linear(in_features=512, out_features=10, bias=True)




```python
#attention combine
attn_combine = nn.Linear(hidden_size * 2, hidden_size)
attn_combine
```




    Linear(in_features=512, out_features=256, bias=True)




```python
out = nn.Linear(hidden_size, output_size)
```

* 디코더 첫번째 SOS 토큰 입력값 만들기


```python
decoder_input = torch.tensor([[SOS_token]])
decoder_input
```




    tensor([[0]])



* decoder_hidden : 첫번째 hiddens state는 인코더의 최종 ouput


```python
# decoder에 처음 입력되는 hidden은 encoder에서 출력된 context vector이다.
decoder_hidden = encoder_hidden
decoder_hidden
```




    tensor([[[ 1.8553e-01, -3.2737e-01, -1.7746e-01,  2.9361e-01, -2.7654e-01,
              -1.6257e-02, -9.3238e-02, -2.9453e-01, -3.6550e-01,  5.1483e-01,
              -4.0575e-01, -2.0769e-01, -1.6290e-01,  2.2070e-02, -2.6099e-01,
              -2.4117e-01, -3.5822e-01,  4.1358e-02, -1.2331e-01,  4.3622e-01,
               5.7500e-02, -3.6290e-01, -3.2169e-01,  2.4425e-01, -4.3693e-02,
              -1.2849e-01,  3.4084e-01,  3.0567e-01,  7.5472e-02,  7.5786e-02,
              -1.7097e-02, -2.7780e-02, -1.0886e-01,  7.9470e-02,  7.5075e-02,
              -4.5910e-01,  3.1378e-02,  3.5593e-01, -8.4244e-02, -8.6812e-02,
              -1.5236e-01, -1.3729e-01, -1.2754e-01,  1.9763e-01,  1.7052e-02,
               3.7005e-01,  1.2101e-01,  1.6412e-01, -2.0437e-01, -2.6039e-01,
              -4.8333e-01,  2.3079e-01,  1.1424e-02,  1.4952e-01, -7.5544e-02,
              -4.1887e-01,  2.0382e-01, -3.5353e-01,  1.1993e-01,  3.8428e-01,
               6.6012e-02, -2.4601e-01,  1.0371e-01,  1.9052e-02, -6.6556e-02,
              -4.0570e-01,  9.0233e-02, -7.5007e-02,  3.6542e-01, -2.5494e-01,
               5.0446e-01,  4.4830e-01, -1.7160e-01,  2.0022e-01,  5.0578e-01,
              -8.6898e-02,  2.6248e-01,  5.7014e-01, -3.3528e-01, -2.7163e-01,
               1.4251e-01,  2.4656e-01,  2.0597e-01, -3.8794e-01,  2.1079e-01,
              -1.3061e-02,  2.8287e-01, -1.1618e-01,  8.0891e-02,  3.1247e-03,
              -1.6827e-01, -2.1751e-02,  2.0819e-01, -2.7487e-01, -9.4698e-02,
               5.1001e-02, -1.3804e-01,  2.1864e-01, -8.7541e-02,  9.2098e-02,
               1.5366e-01,  1.2446e-01, -3.1948e-01, -7.9883e-02, -2.5002e-01,
               1.7433e-01,  3.7732e-01, -5.4707e-01,  3.6940e-02, -2.1193e-01,
              -3.6361e-01,  2.1733e-02, -2.7215e-01, -8.4042e-03, -5.0374e-01,
               5.8897e-01, -8.9980e-02, -3.5412e-01,  3.4306e-01,  7.2969e-02,
               3.9981e-01, -1.9689e-01, -3.6813e-01, -3.0297e-01, -2.4809e-01,
              -1.8404e-01, -3.5193e-01, -2.4280e-01,  5.2934e-01,  2.0030e-01,
               5.1825e-01,  7.3710e-02, -1.6934e-01,  3.9468e-01,  3.3090e-01,
               1.0285e-01,  1.0116e-01,  4.8757e-02,  7.2215e-02, -2.5305e-01,
               3.2151e-01,  1.5773e-01,  7.0490e-02,  3.7154e-02, -2.2654e-01,
               1.9841e-01, -1.5935e-01, -5.9520e-03,  2.6613e-01,  1.0813e-01,
              -2.3759e-01, -1.1725e-01,  1.0883e-01,  5.0219e-01, -1.0235e-01,
              -1.5335e-02, -4.0385e-02, -5.1763e-03,  2.5441e-01,  3.4116e-01,
               2.0091e-01,  3.5100e-01, -7.1971e-01, -4.6089e-01, -3.6717e-01,
              -4.0999e-02,  3.8707e-01, -2.5014e-01,  1.3851e-01,  5.2093e-01,
              -2.6153e-01, -3.0655e-01, -3.4004e-01, -2.6730e-01,  1.9310e-01,
              -3.7675e-02, -1.5948e-01,  3.9838e-01, -6.4142e-02,  1.8905e-01,
              -3.0410e-02, -3.4934e-01, -2.1973e-01,  1.3757e-01, -5.5193e-01,
               3.0144e-01, -1.5491e-01,  3.9494e-01, -1.9529e-01,  8.4889e-02,
              -2.4529e-01, -1.1893e-01,  6.2899e-02,  4.7063e-01, -3.3886e-01,
              -4.0528e-01,  7.6353e-02, -1.9737e-01,  3.5344e-01, -9.3965e-02,
              -2.2042e-01, -2.5712e-01, -4.1075e-01,  1.0953e-01,  1.7969e-01,
              -5.0786e-02,  3.3239e-04,  3.8911e-01, -2.4529e-01, -1.0148e-01,
              -4.6527e-02,  5.1050e-01,  1.1269e-02,  5.1544e-01,  2.5713e-01,
               1.4458e-01,  1.4033e-01, -2.0526e-01,  5.3335e-01,  2.2515e-01,
               3.1177e-01,  5.5605e-01, -5.4880e-01, -4.4740e-02,  1.1625e-01,
              -4.0010e-01,  4.5902e-01, -1.5089e-02,  1.4426e-01, -4.0610e-01,
               1.0105e-01,  3.6140e-01,  4.3310e-01,  8.4004e-02, -6.2668e-01,
              -1.6764e-03, -1.0335e-03,  6.7559e-02, -2.3461e-01, -2.0065e-01,
              -4.5580e-01, -1.6117e-02,  6.5196e-02, -2.4422e-01, -2.0991e-01,
              -1.2629e-01, -3.1711e-01, -1.6199e-01,  1.9908e-01,  2.7948e-01,
              -2.5601e-03,  4.6142e-01,  1.8507e-01, -4.6472e-01, -4.4442e-03,
              -4.2875e-01]]], grad_fn=<StackBackward0>)




```python
# RNN의 첫 입력값인 SOS 토큰을 임베딩 후 dropout을 적용한다.
decoder_embedded = decoder_embedding(decoder_input).view(1,1,-1)
decoder_embedded = dropout(decoder_embedded)
decoder_embedded
```




    tensor([[[ 8.6903e-01,  1.7354e+00,  4.2700e-01, -1.1472e+00, -2.4283e+00,
               9.0323e-02,  1.7649e+00, -4.3337e-02, -2.2300e+00, -5.9895e-01,
              -1.2797e+00, -1.3895e+00, -5.8978e-01,  1.1077e+00, -1.0980e-01,
              -2.7446e+00,  9.1430e-01,  6.2423e-01,  2.4137e+00, -2.2907e-01,
              -6.8633e-02,  2.5096e-02,  1.0036e+00,  0.0000e+00,  5.6463e-01,
              -1.0815e+00, -2.2506e+00, -7.6832e-02,  2.7149e-01,  7.5214e-01,
              -2.3974e-01, -2.4504e-01,  2.8858e+00,  4.2053e-01, -3.7766e-01,
              -6.8074e-01, -1.3725e-01,  4.3047e-01, -1.3843e+00,  2.4304e+00,
               0.0000e+00,  3.4347e-02,  2.7435e-01, -0.0000e+00, -0.0000e+00,
              -2.9426e-02, -1.8375e-01,  5.7472e-01,  6.1659e-01, -1.3018e+00,
              -3.7993e-01, -1.8310e+00,  3.7711e-02, -0.0000e+00,  0.0000e+00,
               1.2380e+00,  2.8910e+00, -9.2709e-02,  1.6688e-01, -6.2623e-01,
               7.0644e-01,  4.0571e-01,  6.2116e-01, -1.6380e-01, -1.9447e-01,
               1.5601e-01, -0.0000e+00, -4.3129e-01,  2.7681e+00,  7.0898e-04,
              -3.9320e-01,  4.3516e-01, -2.1400e+00, -1.5438e+00, -0.0000e+00,
               4.4676e-01,  1.1101e+00, -1.0191e+00,  4.7763e-01,  2.5961e+00,
              -4.8456e-01,  1.1323e+00,  3.8507e-01,  1.5656e+00, -1.4805e+00,
              -1.1995e+00, -9.7418e-01,  0.0000e+00,  7.1803e-01,  0.0000e+00,
              -9.0441e-01,  1.2495e+00, -1.0358e+00,  1.0889e+00,  1.8716e+00,
              -7.7931e-01,  7.3851e-01, -9.4630e-01,  1.2255e+00, -1.9631e+00,
               1.3799e-01, -6.8307e-01, -1.4733e+00, -8.9116e-01,  1.7283e+00,
              -2.8519e+00,  1.7900e+00, -1.6264e+00,  4.4136e-01,  5.5862e-01,
               7.5769e-01, -0.0000e+00,  4.7158e-01,  0.0000e+00,  7.2322e-01,
              -3.9041e-01, -1.0412e+00, -9.9369e-01, -6.1639e-01, -7.8190e-01,
               7.9609e-01, -0.0000e+00, -8.8028e-01, -0.0000e+00,  3.3486e-01,
              -0.0000e+00, -1.0692e+00,  3.3625e-02,  1.1537e+00,  1.6437e+00,
              -9.4654e-01,  4.7388e-01, -8.1023e-02, -1.3475e+00,  2.2122e-01,
               3.9348e-01, -7.2860e-01,  1.4318e-01, -2.7751e-01, -1.3873e+00,
               2.2904e+00,  5.9824e-02, -2.5415e+00, -1.0277e+00, -3.5788e-01,
               2.7489e-01,  5.8688e-01, -1.6669e+00, -1.6840e+00,  3.0936e-01,
              -1.5638e+00,  6.2088e-01,  8.9807e-01, -7.8498e-01, -1.0874e+00,
               1.0929e-02, -7.1622e-01,  0.0000e+00,  5.5568e-01, -3.0230e+00,
               4.6427e-01, -1.6550e+00, -3.5426e-01,  5.8200e-01, -4.0111e-01,
              -1.5226e+00, -6.6196e-01,  1.1026e+00,  2.0984e+00, -0.0000e+00,
               6.7102e-01, -5.7540e-01, -2.2602e-01, -1.0143e+00,  0.0000e+00,
               4.5598e-01,  2.6474e-01, -1.3059e+00, -0.0000e+00,  7.9607e-01,
              -1.1158e+00,  6.1803e-01,  0.0000e+00,  0.0000e+00, -2.9984e-01,
               1.4853e-01, -1.1153e+00, -3.3825e-01,  2.3594e-01, -3.4971e+00,
               9.8115e-01,  0.0000e+00, -1.3827e+00,  3.5527e-01, -1.2069e+00,
              -1.3649e+00,  3.5369e-01, -0.0000e+00,  6.3809e-01, -2.7258e-01,
               8.9946e-01,  1.1100e+00,  2.7813e-01,  2.2703e-01, -8.6998e-01,
               3.3595e-01,  0.0000e+00, -6.9573e-01, -1.4399e+00, -1.7679e+00,
               6.4797e-01,  1.5929e-01, -2.9809e-01, -8.0552e-01, -1.0342e+00,
              -4.3371e-01,  7.7511e-01, -3.7862e-01,  6.0024e-01,  2.2931e+00,
              -9.6925e-01, -1.0829e+00,  1.5989e+00,  1.6653e+00,  4.2369e-01,
              -7.4755e-01,  1.6618e-01,  6.2926e-02,  2.3212e+00,  2.6362e-01,
               4.6440e-01,  5.4052e-01,  1.7119e+00, -7.7613e-01,  2.1085e+00,
               1.4123e+00,  4.1349e-02,  8.4646e-01,  3.0114e-01, -7.3356e-01,
               0.0000e+00,  1.1139e-01,  2.8350e+00, -1.1241e+00,  9.7369e-01,
               0.0000e+00, -1.6071e+00,  1.5802e+00,  4.2332e-01,  1.6350e-01,
              -0.0000e+00, -0.0000e+00,  2.3922e+00, -2.5010e-01,  0.0000e+00,
              -0.0000e+00]]], grad_fn=<MulBackward0>)



* 어텐션 
디코더 임베딩(256) 값과 인코더의 최종 output(256)을 결합 후 attention layer(512)에 입력한다. 인코더의 최종 output은 전체 문장의 문맥 정보를 갖고 있다. 디코더 입력과의 연산을 통해 다음 단어 출력시 인코더의 어느 부분에 집중할 지에 대해 고려할 수 있다. softmax를 거쳐 어디에 집중을 할지에 대한 비율이 출력된다.

![image](https://user-images.githubusercontent.com/101251439/167294783-c0428b93-485d-418b-b1b1-76499e4ae3c8.png)


```python
decoder_attention = attn(torch.cat((decoder_embedded[0], decoder_hidden[0]), 1))
decoder_attention
```




    tensor([[-0.3025,  0.5066,  0.0562, -0.1150,  0.4674,  0.2874,  0.5108,  0.0790,
             -0.8283,  0.2255]], grad_fn=<AddmmBackward0>)




```python
attn_weight = F.softmax(decoder_attention, dim=1)
attn_weight
```




    tensor([[0.0631, 0.1417, 0.0903, 0.0761, 0.1362, 0.1138, 0.1423, 0.0924, 0.0373,
             0.1070]], grad_fn=<SoftmaxBackward0>)



인코더의 각 output에 어텐션을 적용하여 어느 output에 집중할지 계산한다. 예를 들어 "I am a student" 가 있을 때 encoder의 총 output은 4개를 갖게된다. 각 output은 다음과 같은 정보를 갖고 있다. 

1. 첫번째 output: I
2. 두번째 output: I am
3. 세번째 output: I am a 
4. 네번째 output: I am a student

4개의 output 중 어디에 집중을 할지는 attention wight을 곱하여 가장 높은 점수의 output에 집중하게 된다. 인코더의 각 output에 어텐션을 적용하여 attn_applied를 구한다.


```python
attn_applied = torch.bmm(attn_weight.unsqueeze(0), encoder_outputs.unsqueeze(0))
attn_applied
```




    tensor([[[-0.2038,  0.2284, -0.0172,  0.0805, -0.0537,  0.0781, -0.0471,
              -0.0059,  0.1504,  0.1661, -0.0348, -0.1466,  0.0420,  0.0879,
               0.0175, -0.0273, -0.3623,  0.1270, -0.2731,  0.0968, -0.0994,
              -0.1236, -0.1177, -0.1458, -0.2581, -0.0099,  0.0826, -0.1984,
               0.3341,  0.1162,  0.1301, -0.1599, -0.2528, -0.0453, -0.2783,
              -0.1302,  0.0562,  0.0354,  0.2389, -0.1730,  0.0317, -0.0515,
               0.2313, -0.0363, -0.1642,  0.1484,  0.0613,  0.0418,  0.1846,
              -0.0753, -0.3043,  0.0563, -0.0286, -0.1338, -0.0197, -0.0507,
               0.1117, -0.1336,  0.3326,  0.0676,  0.1393, -0.1731,  0.4065,
               0.0574, -0.0368, -0.0194,  0.2230,  0.1812,  0.1362,  0.1259,
               0.0841, -0.0642, -0.1908,  0.1496,  0.1728,  0.1307, -0.0377,
               0.4715, -0.0162, -0.3498, -0.1698,  0.2358,  0.2808, -0.2156,
              -0.0009, -0.1918,  0.1833,  0.0839, -0.3719, -0.0013, -0.2278,
              -0.0509,  0.0698, -0.1500, -0.0149,  0.0175,  0.1271,  0.2273,
              -0.0905, -0.2624, -0.1503, -0.1259,  0.3681,  0.0981, -0.0253,
              -0.2740, -0.0055, -0.1395,  0.1669,  0.1522,  0.0758,  0.1330,
              -0.0187, -0.0206, -0.0927, -0.0605,  0.1763,  0.1752,  0.2347,
               0.0574,  0.1097, -0.0781,  0.1725,  0.0072, -0.1977, -0.3045,
              -0.0278,  0.3458, -0.1138, -0.1429, -0.0185,  0.0031,  0.0894,
               0.2882,  0.0474, -0.1138, -0.1349, -0.1168,  0.2598, -0.0974,
              -0.0284, -0.0250,  0.1593,  0.2192, -0.1381, -0.2468, -0.0767,
              -0.0038, -0.0331,  0.0273,  0.2581,  0.1931,  0.2568,  0.1457,
              -0.1523, -0.1774, -0.3284,  0.0354,  0.1406,  0.1299, -0.0507,
              -0.0054, -0.0905,  0.0719, -0.3066, -0.0129,  0.2426, -0.3425,
              -0.0630,  0.2971, -0.0883, -0.3389, -0.0197, -0.0467,  0.0254,
              -0.3026,  0.0266,  0.0102, -0.0456, -0.0501, -0.0970,  0.1331,
              -0.1064,  0.1520, -0.2676,  0.3695,  0.2640,  0.2470,  0.0827,
               0.2054,  0.0486,  0.0902,  0.0371,  0.1176, -0.1026, -0.2608,
              -0.2052, -0.0795,  0.0212, -0.0172,  0.2044,  0.0015, -0.1227,
               0.0625,  0.1245,  0.4591, -0.2766, -0.1258, -0.0153,  0.0207,
              -0.0796,  0.0044, -0.0827,  0.1693, -0.0063,  0.1207, -0.1580,
              -0.0871,  0.1082,  0.0504,  0.1654,  0.2896, -0.2404, -0.1619,
               0.0090,  0.0384, -0.1145,  0.2226,  0.3508,  0.0675, -0.0116,
               0.0704, -0.0910,  0.2113, -0.1904,  0.2551,  0.1134, -0.2424,
              -0.1381, -0.0629,  0.0980, -0.1208, -0.0239,  0.0945,  0.0492,
               0.1767, -0.1445,  0.1209,  0.0576,  0.3114,  0.1349,  0.1497,
              -0.0182,  0.1349,  0.1314, -0.0490]]], grad_fn=<BmmBackward0>)



* 실제 다음 단어를 예측하기 위해 방금 구한 Attention Value와 디코더의 입력값과 결합한다. 


```python
output = torch.cat((decoder_embedded[0], attn_applied[0]), 1)
output = attn_combine(output).unsqueeze(0)
output = F.relu(output)
output
```




    tensor([[[2.2032e-03, 0.0000e+00, 4.5133e-01, 5.5458e-01, 3.6334e-01,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2375e-01, 4.1301e-01,
              0.0000e+00, 6.3971e-01, 1.2202e-01, 3.6956e-01, 2.3268e-01,
              0.0000e+00, 0.0000e+00, 2.3699e-01, 0.0000e+00, 0.0000e+00,
              3.4387e-01, 7.3438e-02, 0.0000e+00, 0.0000e+00, 7.6014e-01,
              0.0000e+00, 9.4920e-01, 1.0621e+00, 1.6924e-01, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 5.0330e-01, 2.2878e-01, 4.3727e-03,
              0.0000e+00, 7.3372e-01, 6.7497e-01, 5.6222e-01, 3.7617e-01,
              0.0000e+00, 3.4560e-01, 2.4533e-01, 9.6301e-01, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6069e-01, 2.1328e-01,
              4.2347e-01, 0.0000e+00, 4.2865e-01, 0.0000e+00, 9.5506e-02,
              0.0000e+00, 1.9383e-01, 0.0000e+00, 0.0000e+00, 7.9055e-01,
              0.0000e+00, 9.3617e-02, 0.0000e+00, 0.0000e+00, 6.9324e-02,
              0.0000e+00, 0.0000e+00, 1.4140e-01, 0.0000e+00, 4.9590e-01,
              5.9863e-01, 0.0000e+00, 1.5502e-01, 3.2781e-01, 0.0000e+00,
              4.9026e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              6.4148e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5979e-01,
              6.9900e-02, 4.1691e-01, 3.8302e-01, 0.0000e+00, 0.0000e+00,
              3.9854e-01, 0.0000e+00, 1.1305e-01, 1.0364e-01, 6.2970e-01,
              0.0000e+00, 2.6436e-01, 4.6423e-01, 4.3637e-01, 6.5255e-01,
              2.4588e-01, 0.0000e+00, 6.6618e-01, 0.0000e+00, 4.3748e-02,
              6.2294e-02, 0.0000e+00, 1.2537e+00, 0.0000e+00, 1.4791e-02,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0487e-02,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3216e-01, 8.9785e-01,
              3.4545e-01, 0.0000e+00, 0.0000e+00, 3.2045e-01, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              0.0000e+00, 2.7895e-01, 0.0000e+00, 3.5575e-02, 2.6953e-01,
              1.5309e-01, 4.1285e-01, 6.6752e-01, 3.0899e-01, 0.0000e+00,
              3.7239e-01, 9.6790e-02, 0.0000e+00, 1.6163e-01, 1.2508e-01,
              9.7138e-02, 1.8640e-01, 0.0000e+00, 0.0000e+00, 7.2775e-01,
              0.0000e+00, 1.0880e-01, 0.0000e+00, 0.0000e+00, 9.4188e-01,
              2.6571e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              1.3122e-01, 2.7825e-01, 1.0512e-01, 2.4566e-01, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5884e-01, 3.9299e-01,
              1.0597e-01, 0.0000e+00, 1.9388e-01, 4.1671e-01, 2.5886e-01,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              9.1091e-02, 0.0000e+00, 0.0000e+00, 3.7666e-01, 0.0000e+00,
              6.7035e-01, 6.9183e-01, 0.0000e+00, 3.8010e-01, 7.7810e-01,
              5.7926e-01, 0.0000e+00, 4.9319e-01, 1.5051e-01, 0.0000e+00,
              1.2014e-01, 0.0000e+00, 4.3241e-01, 0.0000e+00, 9.5226e-02,
              0.0000e+00, 0.0000e+00, 4.0291e-01, 2.7231e-02, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 3.5666e-01, 4.9648e-02, 5.9716e-02,
              1.2518e-01, 2.6577e-01, 0.0000e+00, 7.5693e-02, 0.0000e+00,
              5.9567e-01, 1.5982e-04, 7.8046e-02, 0.0000e+00, 5.8818e-01,
              0.0000e+00, 1.9825e-01, 0.0000e+00, 0.0000e+00, 3.5915e-01,
              7.0037e-02, 0.0000e+00, 1.6820e-01, 0.0000e+00, 2.2542e-01,
              9.0805e-01, 3.1986e-01, 6.2399e-01, 3.8038e-01, 0.0000e+00,
              3.0586e-01, 4.6532e-01, 1.8863e-01, 0.0000e+00, 0.0000e+00,
              2.2158e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              8.8422e-01, 0.0000e+00, 6.1103e-01, 0.0000e+00, 0.0000e+00,
              0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
              1.7667e-01]]], grad_fn=<ReluBackward0>)



* Attention을 거쳐 나온 ouput과 첫번째 sos 토큰의 output을 rnn에 입력한다. 이후 나오는 output은 다음 단어의 백터값을 의미한다.
![image](https://user-images.githubusercontent.com/101251439/167295080-328e0b1f-46c6-48fc-a966-44dec191a27e.png)


```python
output, decoder_hidden = decoder_rnn(output, decoder_hidden)
decoder_output = F.log_softmax(out(output[0]), dim=1)
_, topi = decoder_output.topk(1)
print(topi)
```

    tensor([[1912]])
    
